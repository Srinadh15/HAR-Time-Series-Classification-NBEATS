{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678aee9-54e8-4437-93af-8476e61fde78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations\n",
    "import random  # For random operations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "# Import necessary components from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  # For scaling features and encoding labels\n",
    "from sklearn.model_selection import StratifiedKFold  # For stratified cross-validation\n",
    "from sklearn.utils.class_weight import compute_class_weight  # For computing class weights to handle imbalances\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix  # For performance evaluation\n",
    "\n",
    "# Import necessary components from Keras\n",
    "from keras.utils import to_categorical  # For converting labels to one-hot encoding\n",
    "from keras.models import Model  # For defining the model\n",
    "from keras.layers import Input, Dense, Dropout  # For defining model layers\n",
    "from keras.callbacks import EarlyStopping  # For early stopping during training\n",
    "from keras.optimizers import Adam  # For optimization\n",
    "import keras_tuner as kt  # For hyperparameter tuning\n",
    "\n",
    "# Define the labels for the activities\n",
    "labels = ['jogging', 'stretching (triceps)', 'jogging (rotating arms)',\n",
    "          'stretching (lunging)', 'jogging (skipping)', 'push-ups',\n",
    "          'push-ups (complex)', 'jogging (sidesteps)',\n",
    "          'stretching (shoulders)', 'sit-ups', 'stretching (hamstrings)',\n",
    "          'sit-ups (complex)', 'lunges', 'burpees',\n",
    "          'stretching (lumbar rotation)', 'jogging (butt-kicks)',\n",
    "          'bench-dips', 'lunges (complex)']\n",
    "\n",
    "# Function to load and preprocess a single CSV file\n",
    "def load_and_preprocess_single_csv(csv_file, scaler=None, label_encoder=None, fit_scaler=False, fit_label_encoder=False):\n",
    "    data = pd.read_csv(csv_file)  # Load the data from the CSV file\n",
    "    \n",
    "    # Clean the data: drop rows with NaN values in 'label' and interpolate remaining NaNs\n",
    "    data = data.dropna(subset=['label'])\n",
    "    data = data.interpolate(method='linear', limit_direction='forward', axis=0).dropna()\n",
    "    \n",
    "    X = data.drop(['sbj_id', 'label'], axis=1).values  # Extract features\n",
    "    y = data['label'].values  # Extract labels\n",
    "    \n",
    "    if fit_scaler:  # Fit and transform the scaler if required\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:  # Otherwise, just transform the features\n",
    "        X = scaler.transform(X)\n",
    "    \n",
    "    if fit_label_encoder:  # Fit and transform the label encoder if required\n",
    "        y = to_categorical(label_encoder.fit_transform(y))\n",
    "    else:  # Otherwise, just transform the labels\n",
    "        y = to_categorical(label_encoder.transform(y))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Function to build the N-BEATS model\n",
    "def build_nbeats_model(hp, input_dim, forecast_length):\n",
    "    input_layer = Input(shape=(input_dim,))  # Define the input layer with the shape of input_dim\n",
    "    units = hp.Int('units', min_value=64, max_value=256, step=64)  # Hyperparameter for the number of units in Dense layers\n",
    "    num_blocks = hp.Int('num_blocks', min_value=2, max_value=4, step=1)  # Hyperparameter for the number of blocks\n",
    "    num_layers = hp.Int('num_layers', min_value=2, max_value=4, step=1)  # Hyperparameter for the number of layers within each block\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)  # Hyperparameter for dropout rate\n",
    "\n",
    "    x = input_layer\n",
    "    for _ in range(num_blocks):  # Iterate over the number of blocks\n",
    "        for _ in range(num_layers):  # Iterate over the number of layers in each block\n",
    "            x = Dense(units, activation='relu')(x)  # Add Dense layer with ReLU activation\n",
    "            x = Dropout(dropout_rate)(x)  # Add Dropout layer\n",
    "    \n",
    "    # Forecasting layer with softmax activation\n",
    "    forecast = Dense(forecast_length, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=forecast)  # Define the model\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')  # Hyperparameter for learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the model\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(csv_files, n_splits=5, max_trials=10, executions_per_trial=1):\n",
    "    random.shuffle(csv_files)  # Shuffle the CSV files\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    # Initialize the scaler and label encoder\n",
    "    scaler = StandardScaler()\n",
    "    label_encoder = LabelEncoder()\n",
    "    for csv_file in csv_files:\n",
    "        X, y = load_and_preprocess_single_csv(csv_file, scaler, label_encoder, fit_scaler=True, fit_label_encoder=True)  # Load and preprocess data\n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "    \n",
    "    all_X = np.concatenate(all_X, axis=0)  # Concatenate all features\n",
    "    all_y = np.concatenate(all_y, axis=0)  # Concatenate all labels\n",
    "\n",
    "    # Cross-validation setup\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    test_accuracies = []\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    for train_index, test_index in kf.split(all_X, np.argmax(all_y, axis=1)):\n",
    "        X_train, X_test = all_X[train_index], all_X[test_index]\n",
    "        y_train, y_test = all_y[train_index], all_y[test_index]\n",
    "\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))  # Compute class weights\n",
    "        class_weights = {i: weight for i, weight in enumerate(class_weights)}  # Create a dictionary of class weights\n",
    "\n",
    "        # Define the tuner for hyperparameter tuning\n",
    "        tuner = kt.RandomSearch(\n",
    "            hypermodel=lambda hp: build_nbeats_model(hp, input_dim=X_train.shape[1], forecast_length=y_train.shape[1]),\n",
    "            objective='val_accuracy',\n",
    "            max_trials=max_trials,\n",
    "            executions_per_trial=executions_per_trial,\n",
    "            directory='hyperparam_tuning',\n",
    "            project_name='nbeats_wearable_data'\n",
    "        )\n",
    "\n",
    "        # Early stopping callback\n",
    "        stop_early = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Perform hyperparameter search\n",
    "        tuner.search(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[stop_early], class_weight=class_weights)\n",
    "\n",
    "        best_model = tuner.get_best_models(num_models=1)[0]  # Get the best model\n",
    "\n",
    "        _, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)  # Evaluate the best model on the test set\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        y_pred = np.argmax(best_model.predict(X_test), axis=1)  # Make predictions\n",
    "        all_predictions.append(y_pred)\n",
    "        all_true_labels.append(np.argmax(y_test, axis=1))\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    overall_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "    # Flatten predictions and true labels for classification report\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_true_labels = np.concatenate(all_true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    average_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "    average_recall = recall_score(all_true_labels, all_predictions, average='weighted')\n",
    "    average_precision = precision_score(all_true_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(\"\\nOverall Test Accuracy:\", overall_accuracy)\n",
    "    for i, test_accuracy in enumerate(test_accuracies):\n",
    "        print(f\"Fold {i + 1} Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Overall Test Accuracy: {overall_accuracy:.4f}\")\n",
    "    print(f\"Average Precision Score: {average_precision:.4f}\")\n",
    "    print(f\"Average Recall Score: {average_recall:.4f}\")\n",
    "    print(f\"Average F1 Score: {average_f1:.4f}\")\n",
    "\n",
    "    # Print overall classification report\n",
    "    print(\"\\nOverall Classification Report:\")\n",
    "    print(\"Average metrics weighted by support across folds:\")\n",
    "    print(classification_report(all_true_labels, all_predictions, digits=4))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_true_labels, all_predictions)\n",
    "\n",
    "    # Plot confusion matrix with annotations\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > np.max(conf_matrix) / 2. else \"black\")\n",
    "\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# Loading the files\n",
    "if __name__ == \"__main__\":\n",
    "    # List of all CSV files\n",
    "    csv_files = [\n",
    "        'data/wear/raw/inertial/sbj_0.csv',\n",
    "        'data/wear/raw/inertial/sbj_1.csv',\n",
    "        'data/wear/raw/inertial/sbj_2.csv',\n",
    "        'data/wear/raw/inertial/sbj_3.csv',\n",
    "        'data/wear/raw/inertial/sbj_4.csv',\n",
    "        'data/wear/raw/inertial/sbj_5.csv',\n",
    "        'data/wear/raw/inertial/sbj_6.csv',\n",
    "        'data/wear/raw/inertial/sbj_7.csv',\n",
    "        'data/wear/raw/inertial/sbj_8.csv',\n",
    "        'data/wear/raw/inertial/sbj_9.csv',\n",
    "        'data/wear/raw/inertial/sbj_10.csv',\n",
    "        'data/wear/raw/inertial/sbj_11.csv',\n",
    "        'data/wear/raw/inertial/sbj_12.csv',\n",
    "        'data/wear/raw/inertial/sbj_13.csv',\n",
    "        'data/wear/raw/inertial/sbj_14.csv',\n",
    "        'data/wear/raw/inertial/sbj_15.csv',\n",
    "        'data/wear/raw/inertial/sbj_16.csv',\n",
    "        'data/wear/raw/inertial/sbj_17.csv',\n",
    "    ]\n",
    "\n",
    "    # Perform training and testing with different hyperparameters\n",
    "    train_and_evaluate(csv_files, n_splits=5, max_trials=10, executions_per_trial=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fee31-0479-4a7c-a552-86a3f93c9966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
